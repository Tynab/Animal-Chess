{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yamia\\AppData\\Local\\Temp\\ipykernel_3432\\1888600386.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yamia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('animal_chess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "board     ----r-E-Tl-----WC--d---------p------L--R--P---...\n",
       "side                                                     -1\n",
       "piece                                                     L\n",
       "atk                                                       0\n",
       "move                                                   F1E1\n",
       "river                                                     0\n",
       "trap                                                      1\n",
       "den                                                       0\n",
       "score                                                   -90\n",
       "winner                                                    0\n",
       "Name: 32, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df.iloc[32]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the piece\n",
    "def encode_piece(piece_char):\n",
    "    piece_mapping = {'-': 0, 'r': 1, 'c': 2, 'd': 3, 'w': 4, 'p': 5, 't': 6, 'l': 7, 'e': 8, 'R': -1, 'C': -2, 'D': -3, 'W': -4, 'P': -5, 'T': -6, 'L': -7, 'E': -8}\n",
    "    return piece_mapping.get(piece_char, 0)\n",
    "\n",
    "encode_piece(sample['piece'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  7.,  0.,  0., -7.,  0.,  0.],\n",
       "       [ 0.,  0.,  3.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  5.,  0.,  0.,  4.],\n",
       "       [ 0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-8., -4.,  0.,  0., -5.,  0.,  0.],\n",
       "       [ 0., -2.,  0.,  0.,  0., -3.,  0.],\n",
       "       [-6.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the board\n",
    "def encode_board(board_str):\n",
    "    board_matrix = numpy.zeros((9, 7))\n",
    "    for i, piece in enumerate(board_str[::-1]):\n",
    "        row, col = divmod(i, 9)\n",
    "        board_matrix[col][row] = encode_piece(piece)\n",
    "    return numpy.flip(numpy.flip(board_matrix, 0), 1)\n",
    "\n",
    "encode_board(sample['board'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  0.,  0., ...,  0.,  0., -7.],\n",
       "       [ 7.,  0.,  0., ...,  0.,  0., -7.],\n",
       "       [ 7.,  0.,  0., ...,  0.,  0., -7.],\n",
       "       ...,\n",
       "       [ 7.,  0.,  0., ...,  0.,  0., -7.],\n",
       "       [ 7.,  0.,  0., ...,  0.,  0., -7.],\n",
       "       [ 7.,  0.,  0., ...,  0.,  0., -7.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['board_encoded'] = df['board'].apply(encode_board)\n",
    "board_matrix_flattened = numpy.array(df['board_encoded'].tolist()).reshape(count, -1)\n",
    "board_matrix_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.array(df['board_encoded'].tolist()).reshape(count, -1)\n",
    "y = df['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 9, 7, 64)          640       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 9, 7, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 4, 3, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 3, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 4, 3, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 2, 1, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              263168    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 339457 (1.29 MB)\n",
      "Trainable params: 339073 (1.29 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "157/157 [==============================] - 8s 36ms/step - loss: 11854.9531 - mae: 55.0982 - val_loss: 13334.6738 - val_mae: 61.4545 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 6651.8579 - mae: 32.7554 - val_loss: 8504.1768 - val_mae: 42.4234 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 4246.9238 - mae: 28.6119 - val_loss: 4955.9653 - val_mae: 29.9788 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 2692.1006 - mae: 24.7869 - val_loss: 3014.4226 - val_mae: 25.2557 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1867.3822 - mae: 21.5588 - val_loss: 2179.1746 - val_mae: 22.3070 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 1506.0963 - mae: 19.7555 - val_loss: 1915.9327 - val_mae: 22.7182 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 20s 126ms/step - loss: 1299.1638 - mae: 18.6116 - val_loss: 1736.7378 - val_mae: 21.3815 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 1155.8438 - mae: 17.6269 - val_loss: 1473.1674 - val_mae: 18.4570 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 1079.8361 - mae: 16.9865 - val_loss: 1380.2644 - val_mae: 18.0204 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 18s 117ms/step - loss: 988.1024 - mae: 16.0822 - val_loss: 1268.0209 - val_mae: 17.3047 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 912.1606 - mae: 15.7235 - val_loss: 1202.1003 - val_mae: 16.5386 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 21s 132ms/step - loss: 874.2755 - mae: 15.1833 - val_loss: 1169.6786 - val_mae: 16.7245 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "157/157 [==============================] - 19s 124ms/step - loss: 818.8869 - mae: 14.8424 - val_loss: 1092.5049 - val_mae: 16.0283 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "157/157 [==============================] - 20s 131ms/step - loss: 781.7435 - mae: 14.5812 - val_loss: 1043.0507 - val_mae: 15.5419 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 740.2084 - mae: 14.0803 - val_loss: 1022.8301 - val_mae: 15.8411 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 713.3259 - mae: 14.1282 - val_loss: 977.5847 - val_mae: 15.2850 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "157/157 [==============================] - 20s 128ms/step - loss: 696.6107 - mae: 13.5214 - val_loss: 958.4956 - val_mae: 14.9706 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 662.8478 - mae: 13.3081 - val_loss: 955.6966 - val_mae: 15.6848 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "157/157 [==============================] - 19s 124ms/step - loss: 646.8940 - mae: 13.0087 - val_loss: 1038.6614 - val_mae: 18.0462 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "157/157 [==============================] - 19s 124ms/step - loss: 618.8370 - mae: 12.7224 - val_loss: 919.6595 - val_mae: 14.8826 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "157/157 [==============================] - 15s 96ms/step - loss: 614.0095 - mae: 12.7653 - val_loss: 905.0460 - val_mae: 15.2746 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 601.8918 - mae: 12.7732 - val_loss: 857.7063 - val_mae: 14.2782 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 584.2318 - mae: 12.3707 - val_loss: 946.8207 - val_mae: 17.0913 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 579.3759 - mae: 12.5224 - val_loss: 835.9188 - val_mae: 13.9571 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 540.9605 - mae: 11.9828 - val_loss: 820.2566 - val_mae: 14.3849 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 533.6320 - mae: 11.7505 - val_loss: 835.3587 - val_mae: 15.1629 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "157/157 [==============================] - 17s 110ms/step - loss: 525.6763 - mae: 11.8736 - val_loss: 771.8628 - val_mae: 13.2268 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "157/157 [==============================] - 18s 116ms/step - loss: 514.5141 - mae: 11.7656 - val_loss: 777.4270 - val_mae: 13.5456 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 499.8153 - mae: 11.6268 - val_loss: 754.8713 - val_mae: 13.3398 - lr: 1.0000e-04\n",
      "Epoch 30/50\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 493.7121 - mae: 11.4353 - val_loss: 763.9634 - val_mae: 13.9272 - lr: 1.0000e-04\n",
      "Epoch 31/50\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 469.9861 - mae: 11.2622 - val_loss: 719.1583 - val_mae: 12.9822 - lr: 1.0000e-04\n",
      "Epoch 32/50\n",
      "157/157 [==============================] - 4s 28ms/step - loss: 472.5912 - mae: 11.1896 - val_loss: 744.7424 - val_mae: 13.9023 - lr: 1.0000e-04\n",
      "Epoch 33/50\n",
      "157/157 [==============================] - 19s 124ms/step - loss: 461.3606 - mae: 11.1104 - val_loss: 719.1309 - val_mae: 13.6196 - lr: 1.0000e-04\n",
      "Epoch 34/50\n",
      "157/157 [==============================] - 13s 83ms/step - loss: 446.0337 - mae: 10.8837 - val_loss: 725.7839 - val_mae: 13.6393 - lr: 1.0000e-04\n",
      "Epoch 35/50\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 450.3910 - mae: 11.0753 - val_loss: 709.1704 - val_mae: 13.0777 - lr: 1.0000e-04\n",
      "Epoch 36/50\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 437.9294 - mae: 10.8029 - val_loss: 691.6414 - val_mae: 12.7458 - lr: 1.0000e-04\n",
      "Epoch 37/50\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 431.8937 - mae: 10.7869 - val_loss: 702.3608 - val_mae: 13.6746 - lr: 1.0000e-04\n",
      "Epoch 38/50\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 430.6571 - mae: 10.7247 - val_loss: 739.3636 - val_mae: 15.2162 - lr: 1.0000e-04\n",
      "Epoch 39/50\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 425.4990 - mae: 10.7657 - val_loss: 679.4446 - val_mae: 13.1162 - lr: 1.0000e-04\n",
      "Epoch 40/50\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 407.2160 - mae: 10.4282 - val_loss: 736.2272 - val_mae: 14.8268 - lr: 1.0000e-04\n",
      "Epoch 41/50\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 401.2386 - mae: 10.4008 - val_loss: 718.3325 - val_mae: 15.0301 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "157/157 [==============================] - 20s 125ms/step - loss: 403.3560 - mae: 10.4787 - val_loss: 637.7329 - val_mae: 12.6809 - lr: 1.0000e-04\n",
      "Epoch 43/50\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 403.1082 - mae: 10.4305 - val_loss: 748.2092 - val_mae: 16.0957 - lr: 1.0000e-04\n",
      "Epoch 44/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 392.0228 - mae: 10.2591 - val_loss: 617.9457 - val_mae: 12.3247 - lr: 1.0000e-04\n",
      "Epoch 45/50\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 384.5851 - mae: 10.3423 - val_loss: 706.8010 - val_mae: 15.3505 - lr: 1.0000e-04\n",
      "Epoch 46/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 366.3808 - mae: 10.0222 - val_loss: 613.1629 - val_mae: 11.7667 - lr: 1.0000e-04\n",
      "Epoch 47/50\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 362.9806 - mae: 9.8309 - val_loss: 627.2128 - val_mae: 12.0838 - lr: 1.0000e-04\n",
      "Epoch 48/50\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 367.4954 - mae: 9.9962 - val_loss: 596.6363 - val_mae: 11.8909 - lr: 1.0000e-04\n",
      "Epoch 49/50\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 349.9629 - mae: 9.7016 - val_loss: 610.4750 - val_mae: 11.8621 - lr: 1.0000e-04\n",
      "Epoch 50/50\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 353.3917 - mae: 9.8070 - val_loss: 612.9191 - val_mae: 12.5874 - lr: 1.0000e-04\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 612.9189 - mae: 12.5874\n",
      "Epoch 1/50\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 412.0898 - mae: 10.5179 - val_loss: 309.3621 - val_mae: 8.2030 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 386.6704 - mae: 10.2106 - val_loss: 307.5306 - val_mae: 8.3152 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 371.9051 - mae: 10.1462 - val_loss: 336.9591 - val_mae: 8.9246 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 357.3192 - mae: 9.9665 - val_loss: 335.9021 - val_mae: 8.8997 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 358.5904 - mae: 9.9466 - val_loss: 322.5975 - val_mae: 8.5105 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 348.1554 - mae: 9.9054 - val_loss: 371.1870 - val_mae: 10.6490 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 4s 28ms/step - loss: 338.3889 - mae: 9.9483 - val_loss: 323.2094 - val_mae: 8.5151 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 13s 86ms/step - loss: 307.1510 - mae: 9.2401 - val_loss: 325.8942 - val_mae: 8.5011 - lr: 2.0000e-05\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 20s 128ms/step - loss: 303.8835 - mae: 9.2425 - val_loss: 316.1209 - val_mae: 8.2297 - lr: 2.0000e-05\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 19s 123ms/step - loss: 296.4463 - mae: 9.0643 - val_loss: 328.0097 - val_mae: 8.8568 - lr: 2.0000e-05\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 7s 47ms/step - loss: 293.6107 - mae: 9.1225 - val_loss: 315.7432 - val_mae: 8.2380 - lr: 2.0000e-05\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 295.6327 - mae: 9.0387 - val_loss: 318.5465 - val_mae: 8.3030 - lr: 2.0000e-05\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 307.5307 - mae: 8.3152\n",
      "Epoch 1/50\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 360.2759 - mae: 9.8556 - val_loss: 286.9283 - val_mae: 7.3892 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 349.9192 - mae: 9.8283 - val_loss: 288.2396 - val_mae: 7.4121 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 348.4363 - mae: 9.6461 - val_loss: 290.1694 - val_mae: 7.4631 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 12s 76ms/step - loss: 347.4685 - mae: 9.7699 - val_loss: 290.3701 - val_mae: 7.4435 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 18s 117ms/step - loss: 340.2645 - mae: 9.6741 - val_loss: 290.6185 - val_mae: 7.4343 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 20s 127ms/step - loss: 330.4343 - mae: 9.5854 - val_loss: 292.5969 - val_mae: 7.5538 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 335.8899 - mae: 9.5420 - val_loss: 292.0593 - val_mae: 7.5375 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 332.6111 - mae: 9.4778 - val_loss: 295.5331 - val_mae: 7.6382 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 17s 111ms/step - loss: 337.2670 - mae: 9.6421 - val_loss: 297.6851 - val_mae: 7.6245 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 336.5468 - mae: 9.5511 - val_loss: 296.8554 - val_mae: 7.5359 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 332.9836 - mae: 9.5779 - val_loss: 297.2944 - val_mae: 7.6099 - lr: 1.0000e-05\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 286.9283 - mae: 7.3892\n",
      "Epoch 1/50\n",
      "157/157 [==============================] - 18s 116ms/step - loss: 370.4922 - mae: 9.8463 - val_loss: 219.2830 - val_mae: 7.2367 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 361.6196 - mae: 9.7097 - val_loss: 223.6495 - val_mae: 7.4453 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 17s 109ms/step - loss: 355.2672 - mae: 9.6631 - val_loss: 222.3651 - val_mae: 7.2842 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 349.1926 - mae: 9.6566 - val_loss: 227.5995 - val_mae: 7.3992 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 14s 87ms/step - loss: 357.0041 - mae: 9.7585 - val_loss: 228.4701 - val_mae: 7.4628 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 358.6108 - mae: 9.5798 - val_loss: 232.8364 - val_mae: 7.6017 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 347.5407 - mae: 9.5535 - val_loss: 228.0074 - val_mae: 7.4295 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 347.6118 - mae: 9.6230 - val_loss: 230.5930 - val_mae: 7.5618 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 354.0377 - mae: 9.5582 - val_loss: 234.8742 - val_mae: 7.6025 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 338.8812 - mae: 9.5072 - val_loss: 233.7347 - val_mae: 7.5284 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 18s 116ms/step - loss: 344.4519 - mae: 9.4915 - val_loss: 230.3098 - val_mae: 7.5414 - lr: 1.0000e-05\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 219.2830 - mae: 7.2367\n",
      "Epoch 1/50\n",
      "157/157 [==============================] - 16s 100ms/step - loss: 361.8937 - mae: 9.7414 - val_loss: 200.4940 - val_mae: 7.1208 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 19s 118ms/step - loss: 363.0963 - mae: 9.7228 - val_loss: 201.8229 - val_mae: 7.1011 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 360.7869 - mae: 9.7082 - val_loss: 205.1687 - val_mae: 7.1441 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 360.7152 - mae: 9.7061 - val_loss: 204.5326 - val_mae: 7.2062 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 351.6660 - mae: 9.5747 - val_loss: 205.2988 - val_mae: 7.1907 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 357.7838 - mae: 9.6435 - val_loss: 209.5677 - val_mae: 7.3698 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 351.0027 - mae: 9.6496 - val_loss: 213.2330 - val_mae: 7.5498 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 354.4283 - mae: 9.6479 - val_loss: 207.0235 - val_mae: 7.2307 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 349.5468 - mae: 9.5960 - val_loss: 208.9766 - val_mae: 7.3376 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 12s 80ms/step - loss: 344.8092 - mae: 9.5900 - val_loss: 210.0536 - val_mae: 7.2921 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 351.0003 - mae: 9.5691 - val_loss: 212.3237 - val_mae: 7.4029 - lr: 1.0000e-05\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 200.4940 - mae: 7.1208\n",
      "Fold results: [[612.9188842773438, 12.587384223937988], [307.5306701660156, 8.315202713012695], [286.9283142089844, 7.389155387878418], [219.28302001953125, 7.236706256866455], [200.49398803710938, 7.1207709312438965]]\n",
      "Average result: [325.43097534   8.5298439 ]\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.0001), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_model((9, 7, 1))\n",
    "model.summary()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "tensorboard_callback = TensorBoard(log_dir=os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\")), histogram_freq=1)\n",
    "checkpoint = ModelCheckpoint(\"best_model_0.h5\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=5).split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    history = model.fit(X_train.reshape(-1, 9, 7, 1), y_train, epochs=50, batch_size=512, validation_data=(X_test.reshape(-1, 9, 7, 1), y_test), callbacks=[reduce_lr, early_stopping, tensorboard_callback, checkpoint])\n",
    "    results.append(model.evaluate(X_test.reshape(-1, 9, 7, 1), y_test))\n",
    "\n",
    "print(\"Fold results:\", results)\n",
    "print(\"Average result:\", numpy.mean(results, axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
